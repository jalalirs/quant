services:
  # Dashboard-only service
  dashboard:
    build:
      context: ..
      dockerfile: docker/Dockerfile.dashboard
    container_name: quant-dashboard
    volumes:
      - ../results:/app/results
      - ../configs:/app/configs

    environment:
      - DASHBOARD_ONLY=1
    command: ["python", "quant.py", "--config", "configs/dashboard_generation.yml"]
    restart: "no"
    
  # CPU-only LLM pipeline (server mode - no benchmarking)
  llm-cpu:
    build:
      context: ..
      dockerfile: docker/Dockerfile.llm-cpu
    container_name: quant-llm-cpu
    volumes:
      - ../models:/app/models
      - ../model_cache:/app/model_cache
      - ../results:/app/results
      - ../configs:/app/configs
    ports:
      - "6001:6001"
    environment:
      - CUDA_VISIBLE_DEVICES=""
      - OMP_NUM_THREADS=4
    # Use server-only config to prevent restart loop
    command: ["python", "quant.py", "--config", "configs/llama38_cpu_only.yml"]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6001/health"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 180s  # Increased start period for model loading
      
  # CPU benchmark runner (separate service)
  llm-cpu-benchmark:
    build:
      context: ..
      dockerfile: docker/Dockerfile.llm-cpu
    container_name: quant-llm-cpu-benchmark
    volumes:
      - ../models:/app/models
      - ../model_cache:/app/model_cache
      - ../results:/app/results
      - ../configs:/app/configs
    environment:
      - CUDA_VISIBLE_DEVICES=""
      - OMP_NUM_THREADS=4
    # Use benchmark config
    command: ["python", "quant.py", "--config", "configs/llama38_cpu_benchmark.yml"]
    profiles: ["benchmark"]  # Only start when explicitly requested
    depends_on:
      - llm-cpu
    
  # GPU-enabled LLM pipeline (server mode)
  llm-gpu:
    build:
      context: ..
      dockerfile: docker/Dockerfile.llm-gpu
    container_name: quant-llm-gpu
    volumes:
      - ../models:/app/models
      - ../model_cache:/app/model_cache
      - ../results:/app/results
      - ../configs:/app/configs
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    command: ["python", "quant.py", "--config", "configs/llama38_trt_mxfp4.yml"]
    restart: no
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    profiles: ["gpu"]  # Only start when GPU profile is requested
    
  # GPU benchmark runner (single run and exit)
  llm-gpu-benchmark:
    build:
      context: ..
      dockerfile: docker/Dockerfile.llm-gpu
    container_name: quant-llm-gpu-benchmark
    volumes:
      - ../models:/app/models
      - ../model_cache:/app/model_cache
      - ../results:/app/results
      - ../configs:/app/configs
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    command: ["python", "quant.py", "--config", "configs/llama38_gpu_benchmark.yml"]
    restart: "no"  # Single run only
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    profiles: ["gpu-benchmark"]  # Only start when GPU benchmark profile is requested

  # GPT-OSS-20B Standard Benchmark (MoE kernels, no mxfp4)
  gpt-oss-standard:
    build:
      context: ..
      dockerfile: docker/Dockerfile.llm-gpu
    container_name: quant-gpt-oss-standard
    volumes:
      - ../models:/app/models
      - ../model_cache:/app/model_cache
      - ../results:/app/results
      - ../configs:/app/configs
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    command: ["python", "quant.py", "--config", "configs/gpt_oss_20b_standard.yml", "--verbose"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    profiles: ["gpt-oss-benchmark"]
    restart: "no"  # Run once for benchmarking

  # GPT-OSS-20B MoE-only Benchmark
  gpt-oss-moe-only:
    build:
      context: ..
      dockerfile: docker/Dockerfile.llm-gpu
    container_name: quant-gpt-oss-moe-only
    volumes:
      - ../models:/app/models
      - ../model_cache:/app/model_cache
      - ../results:/app/results
      - ../configs:/app/configs
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    command: ["python", "quant.py", "--config", "configs/gpt_oss_20b_moe_only.yml", "--verbose"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    profiles: ["gpt-oss-benchmark"]
    restart: "no"  # Run once for benchmarking

  # GPT-OSS-20B mxfp4 Benchmark (expected to fail)
  gpt-oss-mxfp4:
    build:
      context: ..
      dockerfile: docker/Dockerfile.llm-gpu
    container_name: quant-gpt-oss-mxfp4
    volumes:
      - ../models:/app/models
      - ../model_cache:/app/model_cache
      - ../results:/app/results
      - ../configs:/app/configs
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    command: ["python", "quant.py", "--config", "configs/gpt_oss_20b_mxfp4.yml", "--verbose"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    profiles: ["gpt-oss-benchmark"]
    restart: "no"  # Run once for benchmarking

  # NVIDIA Llama-4-Scout-17B-FP4 Benchmark
  llama4-scout-fp4:
    build:
      context: ..
      dockerfile: docker/Dockerfile.llm-gpu
    container_name: quant-llama4-scout-fp4
    volumes:
      - ../models:/app/models
      - ../model_cache:/app/model_cache
      - ../results:/app/results
      - ../configs:/app/configs
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    command: ["python", "quant.py", "--config", "configs/llama4_scout_17b_fp4.yml", "--verbose"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    profiles: ["llama4-scout-benchmark"]
    restart: "no"  # Run once for benchmarking

  # NVIDIA Llama-4-Scout-17B-FP4 Long Context Benchmark
  llama4-scout-fp4-long:
    build:
      context: ..
      dockerfile: docker/Dockerfile.llm-gpu
    container_name: quant-llama4-scout-fp4-long
    volumes:
      - ../models:/app/models
      - ../model_cache:/app/model_cache
      - ../results:/app/results
      - ../configs:/app/configs
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    command: ["python", "quant.py", "--config", "configs/llama4_scout_17b_fp4_long_context.yml", "--verbose"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    profiles: ["llama4-scout-benchmark"]
    restart: "no"  # Run once for benchmarking

  # GPU Compatibility Checker
  gpu-compatibility-check:
    build:
      context: ..
      dockerfile: docker/Dockerfile.llm-gpu
    container_name: quant-gpu-check
    volumes:
      - ../results:/app/results
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    command: ["python", "-c", "from quant.utils.gpu_compatibility import check_gpu_compatibility; check_gpu_compatibility()"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    profiles: ["gpu-check"]
    restart: "no"  # Run once

  # File server for results
  file-server:
    image: nginx:alpine
    container_name: quant-files
    volumes:
      - ../results:/usr/share/nginx/html/results
      - ./nginx.conf:/etc/nginx/nginx.conf
    ports:
      - "8082:80"
    restart: unless-stopped
    depends_on:
      - dashboard

networks:
  default:
    name: quant-network