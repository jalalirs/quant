# Full-featured GPU-enabled LLM pipeline container
# Includes CUDA, TensorRT, and all quantization capabilities

FROM nvidia/cuda:12.1-devel-ubuntu22.04

# Set working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    python3-dev \
    gcc \
    g++ \
    wget \
    curl \
    git \
    && rm -rf /var/lib/apt/lists/*

# Create symlinks for python
RUN ln -s /usr/bin/python3 /usr/bin/python

# Upgrade pip
RUN pip install --upgrade pip

# Copy and install requirements
COPY docker/requirements.llm-gpu.txt requirements.txt
RUN pip install --no-cache-dir -r requirements.txt

# Install TensorRT for CUDA 12.1
RUN pip install --no-cache-dir tensorrt-cu12

# Copy application files
COPY quant/ quant/
COPY configs/ configs/
COPY dataset/ dataset/
COPY quant.py .
COPY install_tensorrt.py .

# Create necessary directories
RUN mkdir -p results models model_cache

# Set environment variables
ENV PYTHONPATH=/app
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility

# Expose port for API server
EXPOSE 6001

# Default command - GPU configuration
CMD ["python", "quant.py", "--config", "configs/llama38_trt_mxfp4.yml"]

# Health check
HEALTHCHECK --interval=60s --timeout=30s --start-period=300s --retries=3 \
    CMD curl -f http://localhost:6001/health || exit 1

# Labels
LABEL maintainer="quant-team"
LABEL description="GPU-accelerated LLM quantization pipeline with TensorRT"
LABEL version="1.0"
LABEL usage="docker run --gpus all -p 6001:6001 -v $(pwd)/models:/app/models quant-llm-gpu"

# Volumes for persistent data
VOLUME ["/app/models", "/app/model_cache", "/app/results", "/app/configs"]