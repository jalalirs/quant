# Full-featured GPU-enabled LLM pipeline container
# Includes CUDA, TensorRT, and all quantization capabilities

FROM nvcr.io/nvidia/pytorch:24.12-py3

# Set working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    python3-dev \
    gcc \
    g++ \
    wget \
    curl \
    git \
    && rm -rf /var/lib/apt/lists/*

# Python symlink already exists in TensorRT base image

# Upgrade pip and check pre-installed packages
RUN pip install --upgrade pip

# Check what's pre-installed in PyTorch container
RUN python -c "import torch; print(f'PyTorch version: {torch.__version__}')" || echo "PyTorch not pre-installed"
RUN python -c "import triton; print(f'Triton version: {triton.__version__}')" || echo "Triton not pre-installed"  
RUN python -c "import tensorrt; print(f'TensorRT version: {tensorrt.__version__}')" || echo "TensorRT not pre-installed"

# Copy and install requirements
COPY docker/requirements.llm-gpu.txt requirements.txt
RUN pip install  -r requirements.txt

# TensorRT is pre-installed in the base image

# Create a script to install MegaBlocks at runtime (when CUDA is available)
RUN echo '#!/bin/bash\n\
if nvidia-smi > /dev/null 2>&1; then\n\
    echo "CUDA detected, installing MegaBlocks with compatible PyTorch 2.7.0..."\n\
    pip install megablocks==0.10.0\n\
else\n\
    echo "ERROR: No CUDA detected. GPU container requires NVIDIA runtime."\n\
    exit 1\n\
fi\n\
exec "$@"' > /usr/local/bin/install-cuda-deps.sh && \
chmod +x /usr/local/bin/install-cuda-deps.sh

# Copy application files
COPY quant/ quant/
COPY configs/ configs/
COPY dataset/ dataset/
COPY quant.py .

# Create necessary directories
RUN mkdir -p results models model_cache

# Set environment variables
ENV PYTHONPATH=/app
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility

# Expose port for API server
EXPOSE 6001

# Use entrypoint to install CUDA dependencies at runtime
ENTRYPOINT ["/usr/local/bin/install-cuda-deps.sh"]

# Default command - GPU configuration
CMD ["python", "quant.py", "--config", "configs/llama38_trt_mxfp4.yml"]

# Health check
HEALTHCHECK --interval=60s --timeout=30s --start-period=300s --retries=3 \
    CMD curl -f http://localhost:6001/health || exit 1

# Labels
LABEL maintainer="quant-team"
LABEL description="GPU-accelerated LLM quantization pipeline with TensorRT"
LABEL version="1.0"
LABEL usage="docker run --gpus all -p 6001:6001 -v $(pwd)/models:/app/models quant-llm-gpu"

# Volumes for persistent data
VOLUME ["/app/models", "/app/model_cache", "/app/results", "/app/configs"]