# Full-featured GPU-enabled LLM pipeline container
# Includes CUDA, TensorRT, and all quantization capabilities

FROM nvidia/cuda:12.1.1-devel-ubuntu22.04

# Set working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    python3-dev \
    gcc \
    g++ \
    wget \
    curl \
    git \
    && rm -rf /var/lib/apt/lists/*

# Create symlinks for python
RUN ln -s /usr/bin/python3 /usr/bin/python

# Upgrade pip
RUN pip install --upgrade pip

# Copy and install requirements
COPY docker/requirements.llm-gpu.txt requirements.txt
RUN pip install  -r requirements.txt

# Install TensorRT for CUDA 12.1 - pinned version compatible with PyTorch 2.8.0
RUN pip install tensorrt-cu12==10.6.0

# Create a script to install MegaBlocks at runtime (when CUDA is available)
RUN echo '#!/bin/bash\n\
if nvidia-smi > /dev/null 2>&1; then\n\
    echo "CUDA detected, installing MegaBlocks without dependencies..."\n\
    pip install megablocks --no-deps\n\
    echo "Installing MegaBlocks dependencies manually..."\n\
    pip install stanford-stk==0.7.1\n\
else\n\
    echo "ERROR: No CUDA detected. GPU container requires NVIDIA runtime."\n\
    exit 1\n\
fi\n\
exec "$@"' > /usr/local/bin/install-cuda-deps.sh && \
chmod +x /usr/local/bin/install-cuda-deps.sh

# Copy application files
COPY quant/ quant/
COPY configs/ configs/
COPY dataset/ dataset/
COPY quant.py .

# Create necessary directories
RUN mkdir -p results models model_cache

# Set environment variables
ENV PYTHONPATH=/app
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility

# Expose port for API server
EXPOSE 6001

# Use entrypoint to install CUDA dependencies at runtime
ENTRYPOINT ["/usr/local/bin/install-cuda-deps.sh"]

# Default command - GPU configuration
CMD ["python", "quant.py", "--config", "configs/llama38_trt_mxfp4.yml"]

# Health check
HEALTHCHECK --interval=60s --timeout=30s --start-period=300s --retries=3 \
    CMD curl -f http://localhost:6001/health || exit 1

# Labels
LABEL maintainer="quant-team"
LABEL description="GPU-accelerated LLM quantization pipeline with TensorRT"
LABEL version="1.0"
LABEL usage="docker run --gpus all -p 6001:6001 -v $(pwd)/models:/app/models quant-llm-gpu"

# Volumes for persistent data
VOLUME ["/app/models", "/app/model_cache", "/app/results", "/app/configs"]