# Full-featured GPU-enabled LLM pipeline container
# Includes CUDA, TensorRT, and all quantization capabilities

FROM nvcr.io/nvidia/pytorch:25.02-py3

# Set PyTorch CUDA memory allocator to avoid fragmentation
ENV PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# Set working directory
WORKDIR /app

# System dependencies already installed in NVIDIA PyTorch container

# Python symlink already exists in TensorRT base image

# Upgrade pip and check pre-installed packages
RUN pip install --upgrade pip

# Check what's pre-installed in PyTorch container
RUN python -c "import torch; print(f'PyTorch version: {torch.__version__}')" || echo "PyTorch not pre-installed"
RUN python -c "import triton; print(f'Triton version: {triton.__version__}')" || echo "Triton not pre-installed"  
RUN python -c "import tensorrt; print(f'TensorRT version: {tensorrt.__version__}')" || echo "TensorRT not pre-installed"

# Copy and install requirements
COPY docker/requirements.llm-gpu.txt requirements.txt
RUN pip install -r requirements.txt

# Install MegaBlocks separately with --no-deps to avoid torch conflicts
RUN pip install --no-deps megablocks==0.10.0

# TensorRT is pre-installed in the base image

# Copy application files
COPY quant/ quant/
COPY configs/ configs/
COPY dataset/ dataset/
COPY quant.py .


# Create necessary directories
RUN mkdir -p results models model_cache

# Set environment variables
ENV PYTHONPATH=/app
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility

# Expose port for API server
EXPOSE 6001

# No entrypoint needed - everything installed during build

# Default command - GPU configuration
CMD ["python", "quant.py", "--config", "configs/llama38_trt_mxfp4.yml"]

# Health check
HEALTHCHECK --interval=60s --timeout=30s --start-period=300s --retries=3 \
    CMD curl -f http://localhost:6001/health || exit 1

# Labels
LABEL maintainer="quant-team"
LABEL description="GPU-accelerated LLM quantization pipeline with TensorRT"
LABEL version="1.0"
LABEL usage="docker run --gpus all -p 6001:6001 -v $(pwd)/models:/app/models quant-llm-gpu"

# Volumes for persistent data
VOLUME ["/app/models", "/app/model_cache", "/app/results", "/app/configs"]