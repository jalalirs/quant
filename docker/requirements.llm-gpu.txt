# MXFP4 + Flash Attention 3 optimized requirements
# Base: nvcr.io/nvidia/tritonserver:25.06-trtllm-python-py3
# Pre-installed: PyTorch 2.7.0a0, TensorRT 10.10.0.31, Python 3.12.3

# Core ML dependencies - only missing packages
transformers>=4.55.1
tokenizers>=0.13.0

# Critical: Triton 3.4+ for MXFP4 support (installed in Dockerfile)
# triton>=3.4

# Critical: Kernels library for MXFP4 and Flash Attention 3 (installed in Dockerfile)  
# kernels>=0.5.0

# ONNX dependencies - onnx pre-installed in TensorRT-LLM container
onnxruntime-gpu>=1.22.0

# MegaBlocks dependencies - NOT NEEDED for H100/H200 with MXFP4
# stanford-stk

# Server dependencies
fastapi>=0.100.0
uvicorn[standard]>=0.22.0
pydantic>=2.0.0

# Client dependencies
openai>=1.0.0
aiohttp>=3.8.0

# Utility dependencies - numpy pre-installed
PyYAML>=6.0
packaging>=21.0

# Dashboard dependencies
Jinja2>=3.1.0

# CUDA utilities - pre-installed in TensorRT-LLM container
nvidia-ml-py>=12.535.0

# Progress bars and logging
tqdm>=4.65.0
rich>=13.0.0

# Hugging Face hub
huggingface_hub>=0.16.0

# Accelerate for multi-GPU support - may be pre-installed
accelerate>=0.20.0

# Note: MegaBlocks NOT included - H100/H200 use MXFP4 + Flash Attention 3 instead
# Note: triton and kernels installed in Dockerfile to ensure proper versions