interface:
  type: server_openai
  host: "0.0.0.0"
  port: 6001
  
model:
  model_name: "openai/gpt-oss-20b"
  cache_dir: "./model_cache"
  max_batch_size: 1
  
  # Optimized loader configuration
  loader_type: optimized_loader
  optimizations:
    # Standard inference without mxfp4
    use_mxfp4: false
    use_flash_attention_2: true   # Downgrade to FA2 for CUDA compatibility
    use_megablocks_moe: true      # Enable MoE kernels for speed boost
    use_kernels: true
    torch_dtype: "auto"
    device_map: "auto"
  
  conversion:
    type: onnx
    output_path: "./models/gpt_oss_20b_standard.onnx"
    onnx_export:
      opset_version: 14
      use_dynamic_axes: true
      input_names: ["input_ids"]
      output_names: ["logits"]
      export_sequence_length: 2048
    quantization:
      type: standard
      precision: fp16
      output_path: "./models/gpt_oss_20b_standard.trt"
      max_workspace_size: 8589934592  # 8GB for larger model

generation:
  max_new_tokens: 100
  temperature: 0.7
  top_k: 50

client:
  type: speed_benchmark
  dataset: "dataset/speed_benchmark.json"
  output: "results/gpt_oss_20b_standard_benchmark.json"
  num_requests: 50   # Reduced for large model
  concurrent_requests: 1
  warmup_requests: 3
  
benchmark_metadata:
  model_variant: "gpt-oss-20b"
  optimization_type: "standard_with_moe"
  description: "Benchmark of GPT-OSS-20B with MegaBlocks MoE kernels (no mxfp4)"
  expected_performance: "Should work reliably with MoE optimization"
