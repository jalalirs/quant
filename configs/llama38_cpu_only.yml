# CPU-only configuration for Docker containers
# No TensorRT quantization - uses ONNX runtime only

interface:
  type: server_openai
  host: "0.0.0.0"
  port: 6001
  
model:
  model_name: gpt2  # Simple GPT-2 model for better ONNX compatibility
  cache_dir: "./model_cache"
  max_batch_size: 1       # Single batch for CPU
  conversion:
    type: onnx
    output_path: "./models/model_cpu.onnx"
    # ONNX export settings
    onnx_export:
      opset_version: 14     # Lower opset version for better compatibility
      use_dynamic_axes: true # Let model determine its own shapes
      input_names: ["input_ids"]
      output_names: ["logits"]
      export_sequence_length: 1024  # Use GPT-2's full context length for ONNX export
  
  # Generation settings
  generation:
    max_new_tokens: 50      # Maximum tokens to generate
    temperature: 0.7        # Sampling temperature
    top_k: 50              # Top-k sampling